import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score

from collections import defaultdict
from datetime import datetime as dt
import sys
sys.path.append('..')

from utils import get_logger, get_arguments
from models import train_rf


def load_features(features, _debug):
    """
    return pandas dataframe including only given features
    """
    for suffix in ['train', 'test']:
        dfs = [pd.read_pickle('../../data/features/{0}_{1}.pkl'.format(f, suffix)) for f in features]
        x_all_df = pd.concat(dfs, axis=1)

        if _debug:  # use only 1%
            x_all_df = x_all_df.iloc[:int(x_all_df.shape[0]/100)]

        yield x_all_df


def load_target(target_name, _debug):
    """
    return pandas series of given target_name
    """
    train = pd.read_pickle('../../data/input/train.pkl')
    y_all_se = train[target_name]

    if _debug:  # use only 1%
        y_all_se = y_all_se.iloc[:int(y_all_se.shape[0]/100)]

    return y_all_se


def calc_metrics(scores, y_pr, _y):
    """
    return metrics of given data
    """
    scores['acc'].append(accuracy_score(y_pr, _y))  # accuracy
    scores['auc'].append(roc_auc_score(y_pr, _y))  # area under the curve

    return scores


def plot_importances(importances_df, _title=None):
    """
    return barplot of feature importances
    """
    fig, ax = plt.subplots()
    sns.barplot(x='importance', y='feature', data=importances_df, ax=ax)  # plot
    if _title:  # set title
        ax.set_title(_title)
    fig.tight_layout()  # adjust margin

    return fig


def train_model(config, _debug, logger, start_dt, train_and_predict):
    """
    train model with features. model and features are designated in config
    """
    features = config['features']
    target_name = config['target_name']
    id_name = config['id_name']

    # load features and target
    x_train_all_df, x_test_df = load_features(features, _debug)
    y_train_all_se = load_target(target_name, _debug)

    # save feature names
    feature_names = x_train_all_df.columns.tolist()

    # convert to numpy array
    x_train_all, x_test = x_train_all_df.to_numpy(), x_test_df.to_numpy()
    y_train_all = y_train_all_se.to_numpy()

    logger.debug('x_train_all:{0} x_test:{1}'.format(x_train_all.shape, x_test.shape))
    logger.debug('y_train_all:{0}'.format(y_train_all.shape))

    # load model params
    params = config['params']
    seed = config['seed']
    model_name = config['model_name']

    # generate stratified k-fold instance
    n_splits = config['n_splits']
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

    # to store results
    y_te_prs = np.zeros(len(y_train_all))
    y_preds = np.zeros(x_test.shape[0])
    scores_tr, scores_te = defaultdict(list), defaultdict(list)
    importances_df = pd.DataFrame()

    # cross validation
    for _fold, (tr_idx, te_idx) in enumerate(skf.split(x_train_all, y_train_all)):
        _fold += 1
        logger.debug('------ {0} / {1} fold ------'.format(_fold, n_splits))

        # extract dataset
        x_tr, x_te = x_train_all[tr_idx, :], x_train_all[te_idx, :]
        y_tr, y_te = y_train_all[tr_idx], y_train_all[te_idx]

        logger.debug('x_tr:{0} x_te:{1}'.format(x_tr.shape, x_te.shape))
        logger.debug('y_tr:{0} y_te:{1}'.format(y_tr.shape, x_te.shape))

        # train model
        y_tr_pr, y_te_pr, y_pred, model = train_and_predict(x_tr, y_tr, x_te, x_test, params)

        # save prediction
        y_te_prs[te_idx] += y_te_pr / (n_splits - 1)
        y_preds += y_pred / n_splits

        # compute metric
        scores_tr = calc_metrics(scores_tr, y_tr_pr, y_tr)
        scores_te = calc_metrics(scores_te, y_te_pr, y_te)

        logger.debug('[{0}f] train_acc:{1} test_acc:{2}'.format(
            _fold, scores_tr['acc'][-1], scores_te['acc'][-1]
        ))
        logger.debug('[{0}f] train_auc:{1} test_auc:{2}'.format(
            _fold, scores_tr['auc'][-1], scores_te['auc'][-1]
        ))

        # feature importance
        if hasattr(model, 'feature_importances_'):
            importances_df['{}_fold'.format(_fold)] = model.feature_importances_

    # mean metrics
    scores_cv_tr = np.mean(pd.DataFrame(scores_tr), axis=0)
    scores_cv_te = np.mean(pd.DataFrame(scores_te), axis=0)

    logger.debug('------ cross validation ------')
    logger.debug('[cv] train_acc:{0}, test_acc:{1}'.format(scores_cv_tr['acc'], scores_cv_te['acc']))
    logger.debug('[cv] train_auc:{0}, test_auc:{1}'.format(scores_cv_tr['auc'], scores_cv_te['auc']))

    if not importances_df.empty:
        # mean feature importance
        importances_df = pd.DataFrame({
            'feature': feature_names,
            'importance': np.mean(importances_df, axis=1)
        })

        # save
        file_name = 'importances_{0:%m%d_%H%M%S}_{1:.5f}_{2}'.format(start_dt, scores_cv_te['auc'], model_name)
        importances_df.to_csv(
            '../../data/output/{0}.csv'.format(file_name),
            index=False
        )

        # plot
        fig = plot_importances(importances_df, file_name)
        fig.savefig(
            '../../figures/feature_importance/{0}.png'.format(file_name)
        )

    # save prediction on te dataset
    train_df = pd.read_pickle('../../data/input/train.pkl')
    if _debug:
        train_df = train_df.iloc[:int(train_df.shape[0]/100)]

    y_te_prs_df = pd.DataFrame({
        'id': train_df[id_name],
        'pred': y_te_prs,
        'truth': y_train_all
    })
    logger.debug('y_tr_prs_df:{0}'.format(y_te_prs_df.shape))

    # save
    y_te_prs_df.to_pickle(
        '../../data/output/val_{0:%m%d_%H%M%S}_{1:.5f}_{2}.pkl'.format(
            start_dt, scores_cv_te['auc'], model_name)
    )

    # create submission file
    sub_df = pd.read_pickle('../../data/input/sample_submission.pkl')
    if _debug:
        sub_df = sub_df.iloc[:int(sub_df.shape[0] / 100)]
    sub_df[target_name] = y_preds
    logger.debug('sub_df:{0}'.format(sub_df.shape))

    # save
    sub_df.to_csv(
        '../../data/output/sub_{0:%m%d_%H%M%S}_{1:.5f}_{2}.csv'.format(
            start_dt, scores_cv_te['auc'], model_name),
        index=False
    )


if __name__ == '__main__':
    # set logger
    start_dt = dt.now()
    logger = get_logger(__name__, start_dt)
    logger.debug('start train_model.py')

    # get config
    args = get_arguments()
    config = json.load(open(args.config))
    logger.debug(config)

    # get debag flag
    _debug = args.debug

    # load model
    models = {
        'random_forest': train_rf
    }
    train_and_predict = models[config['model_name']]

    train_model(config, _debug, logger, start_dt, train_and_predict)

    logger.debug('finish train_model.py')
