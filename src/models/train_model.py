import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix, coo_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, roc_auc_score

from collections import defaultdict
from datetime import datetime as dt
import sys
sys.path.append('..')
import gc
gc.enable()
import pickle

from utils import get_logger, get_arguments, load_features, load_target, df_to_matrix
from models import train_rf, train_lr


def calc_metrics(scores, y_pred, y_true):
    """
    return metrics of given data
    """
    scores['acc'].append(accuracy_score(y_true, y_pred))  # accuracy
    scores['auc'].append(roc_auc_score(y_true, y_pred))  # area under the curve

    return scores


def plot_importances(importances_df, _title=None):
    """
    return barplot of feature importances
    """
    fig, ax = plt.subplots(figsize=(10, 5+0.3*len(importances_df)))
    sns.barplot(x='importance', y='feature',
                data=importances_df.sort_values('importance', ascending=False), ax=ax)  # plot
    if _title:  # set title
        ax.set_title(_title)
    fig.tight_layout()  # adjust margin

    return fig


def train_model(config, _debug, logger, start_dt, train_and_predict):
    """
    train model with features. model and features are designated in config
    """
    features = config['features']
    label_name = config['label_name']
    id_name = config['id_name']

    # load only train features and label
    x_train_all = load_features(features, _debug, target='train')
    y_train_all = load_target(label_name, _debug)

    gc.collect()

    logger.debug('x_train_all:{0}'.format(x_train_all.shape))
    logger.debug('y_train_all:{0}'.format(y_train_all.shape))

    # save feature names and index
    feature_names = x_train_all.columns.tolist()
    x_train_idx = x_train_all.index

    # convert from df to matrix
    x_train_all = df_to_matrix(x_train_all)

    # load model params
    params = config['params']
    seed = config['seed']
    model_name = config['model_name']

    # generate stratified k-fold instance
    n_splits = config['n_splits']
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)

    # to store results
    y_te_prs = np.zeros(len(y_train_all))
    scores_tr, scores_te = defaultdict(list), defaultdict(list)
    importances_df = pd.DataFrame()
    trained_models = []

    # cross validation
    for _fold, (tr_idx, te_idx) in enumerate(skf.split(x_train_idx, y_train_all)):
        _fold += 1
        logger.debug('------ {0} / {1} fold ------'.format(_fold, n_splits))

        # extract dataset
        x_tr, x_te = x_train_all[tr_idx, :], x_train_all[te_idx, :]
        y_tr, y_te = y_train_all[tr_idx], y_train_all[te_idx]

        logger.debug('x_tr:{0} x_te:{1}'.format(x_tr.shape, x_te.shape))
        logger.debug('y_tr:{0} y_te:{1}'.format(y_tr.shape, x_te.shape))

        # train model
        y_tr_pr, y_te_pr, model = train_and_predict(x_tr, y_tr, x_te, params)

        # save prediction
        y_te_prs[te_idx] += y_te_pr / (n_splits - 1)

        # compute metric
        scores_tr = calc_metrics(scores_tr, y_tr_pr, y_tr)
        scores_te = calc_metrics(scores_te, y_te_pr, y_te)

        logger.debug('[{0}f] train_acc:{1} test_acc:{2}'.format(
            _fold, scores_tr['acc'][-1], scores_te['acc'][-1]
        ))
        logger.debug('[{0}f] train_auc:{1} test_auc:{2}'.format(
            _fold, scores_tr['auc'][-1], scores_te['auc'][-1]
        ))

        # save model
        trained_models.append(model)

        # feature importance
        if hasattr(model, 'feature_importances_'):
            importances_df['{}_fold'.format(_fold)] = model.feature_importances_
        elif hasattr(model, 'coef_'):
            importances_df['{}_fold'.format(_fold)] = model.coef_.flatten()

        del x_tr, x_te, y_tr, y_te, y_tr_pr, y_te_pr, model
        gc.collect()

    # mean metrics
    scores_cv_tr = np.mean(pd.DataFrame(scores_tr), axis=0)
    scores_cv_te = np.mean(pd.DataFrame(scores_te), axis=0)

    logger.debug('------ cross validation ------')
    logger.debug('[cv] train_acc:{0}, test_acc:{1}'.format(scores_cv_tr['acc'], scores_cv_te['acc']))
    logger.debug('[cv] train_auc:{0}, test_auc:{1}'.format(scores_cv_tr['auc'], scores_cv_te['auc']))

    if importances_df.any(axis=None):
        # mean feature importance
        importances_df = pd.DataFrame({
            'feature': feature_names,
            'importance': np.mean(importances_df, axis=1)
        })

        # save
        file_name = 'importances_{0:%m%d_%H%M%S}_{1:.5f}_{2}'.format(start_dt, scores_cv_te['auc'], model_name)
        importances_df.to_csv(
            '../../data/output/{0}.csv'.format(file_name),
            index=False
        )

        # plot
        fig = plot_importances(importances_df, file_name)
        fig.savefig(
            '../../figures/feature_importance/{0}.png'.format(file_name)
        )

    # save prediction on te dataset
    train_df = pd.read_pickle('../../data/input/train.pkl')
    if _debug:
        train_df = train_df.iloc[:int(train_df.shape[0]/100)]

    y_te_prs_df = pd.DataFrame({
        'id': train_df[id_name],
        'pred': y_te_prs,
        'truth': y_train_all
    })
    logger.debug('y_tr_prs_df:{0}'.format(y_te_prs_df.shape))

    del train_df
    gc.collect()

    # save prediction on cross-validation test
    y_te_prs_df.to_pickle(
        '../../data/output/val_{0:%m%d_%H%M%S}_{1:.5f}_{2}.pkl'.format(
            start_dt, scores_cv_te['auc'], model_name)
    )

    del y_te_prs_df
    gc.collect()

    # save models
    model_path = '../../models/models_{0:%m%d_%H%M%S}_{1:.5f}_{2}.pkl'.format(
            start_dt, scores_cv_te['auc'], model_name)
    with open(model_path, 'wb') as f:
        pickle.dump(trained_models, f)


if __name__ == '__main__':
    # set logger
    start_dt = dt.now()
    logger = get_logger(__name__, start_dt)
    logger.debug('start train_model.py')

    # get config
    args = get_arguments()
    config = json.load(open('../../configs/{}'.format(args.config)))
    logger.debug(config)

    # get debag flag
    _debug = args.debug

    # load model
    models = {
        'random_forest': train_rf,
        'logistic_regression': train_lr
    }
    train_and_predict = models[config['model_name']]

    train_model(config, _debug, logger, start_dt, train_and_predict)

    logger.debug('finish train_model.py')
